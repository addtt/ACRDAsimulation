-- userThruputCurve_varyingNhostSf
See log01 for logs

Each curve has fixed number of hosts, spreading factor and all other parameters. We vary the arrival rate per user to get one curve.
If the interference is not heavy, then the throughput per user increases linearly until the channel capacity is reached and the users are unstable and backlogged, so the throughput saturates.
In the case of SF=1 we see that with 7 hosts the throughput saturates to a lower value, because the interference cancellation is not effective enough. With 10 hosts it's even worse, since the throughput goes to zero before the users reach instability.
For SF=2 and few (5) hosts, the interference is small, thus the throughput saturates to half of the value that we had with SF=1 (the service rate is halved). If we increase the number of hosts to 7 and 10, as we did for SF=1, we see that the interfering packets can be resolved, and the throughput per user has the same behaviour as with nHosts=5.
Increasing the SF yields a worse performance if the interference is small (i.e. if there are few hosts), but as soon as the number of hosts increases, using a higher SF yields better results. The higher the SF, the higher the number of users after which the system crashes.
Note that the interference is small even if we simply have small traffic from each user, but in that case using a higher SF is not a problem from a throughput point of view (the users are still stable even if we increase rho -- the load factor)


-- userThruputCurve_varyingSf
See log02 for logs

We now fix the number of hosts in the whole plot, and for each curve we fix the SF. Then we change the arrival rate for each host, in order to get a throughput curve as function of the arrival rate. In this case the system throughput and the user throughput are proportional.
We observe that with SF=1 and SF=2 the system becomes unstable. With SF>=3 it saturates because the users become backlogged (their buffers become unstable), not because of the interference, which can be resolved.
Since the saturation is due to the users' queues and not to the interference in the system, if we further increase SF the maximum achievable throughput becomes inversely proportional to SF. This is because SF is proportional to the frame length and therefore also to the service time of each user.


-- userThruputCurve_comparisonESSA
See log03 for logs

It's actually pointless to use replicas. The SF is the really useful thing.


-- userThruputCurve_comparisonESSA_60hosts
See log04 for logs

Apparently the same result as with 20 hosts (previous case). The number of hosts may not be the key for finding a scenario in which ACRDA+SF is actually better than E-SSA.


----


If we only look at the system throughput, then we can get better and better results just increasing the SF to get better at IC, decreasing the arrival rate per user in order to avoid instability (the frames are longer), and increasing the number of users so as to get a higher throughput than before.
This is feasible but each user ends up with a very limited throughput.
Another problem is that we need a lot of IC iterations, so if they are limited there is a point after which the situation gets worse.